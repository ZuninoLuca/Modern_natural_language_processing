{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scoring the 'interactions' dataset using ChatGPT"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we prompt ChatGPT to score the candidate answers (determined considering the interaction with the highest BERTScore) taking into account the golden answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install artifacts/gpt_wrapper-0.0.8-py3-none-any.whl\n",
    "%pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gpt_wrapper\n",
    "gpt_wrapper.api_key = \"GPT_API_KEY_REMOVED_FOR_PRIVACY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt_wrapper.chat import Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('complete_complete.json') as json_file:\n",
    "    data = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_and_concatenate(nested_list):\n",
    "    # If the input is a string, return it\n",
    "    if isinstance(nested_list, str):\n",
    "        return nested_list\n",
    "\n",
    "    # If the input is a list, apply the function to each element and concatenate the results\n",
    "    if isinstance(nested_list, list):\n",
    "        return ' '.join(flatten_and_concatenate(element) for element in nested_list)\n",
    "\n",
    "    # If the input is neither a string nor a list, return an empty string\n",
    "    return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import tiktoken\n",
    "\n",
    "start_tokens_used = 1766408\n",
    "used_tokens = 0\n",
    "\n",
    "# Create an empty list to store the datapoints\n",
    "scored = []\n",
    "skipped = []\n",
    "start_index = 0 \n",
    "\n",
    "try:\n",
    "    # Load the previously scored datapoints (useful if the notebook crashes and you need to restart it)\n",
    "    with open('complete_complete_with_grades.json', 'r') as f:\n",
    "        scored = json.load(f)\n",
    "    with open('skipped.json', 'r') as f:\n",
    "        skipped = json.load(f)\n",
    "\n",
    "    start_index = len(scored) + len(skipped)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Failed to load previous data: {e}\")\n",
    "    \n",
    "data_len = len(data)\n",
    "start_time = time.time()\n",
    "elapsed_times = []\n",
    "\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "\n",
    "# Iterate over the data\n",
    "for count, datapoint in enumerate(data[start_index:], start_index + 1):\n",
    "    print(\"########################################################\")\n",
    "    print(\"Processing datapoint\", count, \"of\", data_len, \"(\", round(count/data_len*100, 2), \"%)\")\n",
    "    print(\"########################################################\")\n",
    "\n",
    "    if \"question\" in datapoint and \"answer\" in datapoint and \"candidate_answer\" in datapoint and datapoint[\"question\"] is not None and datapoint[\"answer\"] is not None and datapoint[\"candidate_answer\"] is not None:\n",
    "        question = datapoint[\"question\"]\n",
    "        answer = datapoint[\"answer\"]\n",
    "        candidate = datapoint[\"candidate_answer\"]\n",
    "    else:\n",
    "        print(\"Skipping datapoint\", count, \"because it is missing a question, answer, or candidate_answer\")\n",
    "        skipped.append(count)\n",
    "        with open('skipped.json', 'w') as f:\n",
    "            json.dump(skipped, f)\n",
    "        continue\n",
    "\n",
    "    prompt = \"Given the following question, correct answer, and candidate answer, score the candidate answer on a scale of 0-5, based on how much it matches the correct answer. Just return the number, DO NOT ADD FURTHER COMMENTS.\\n\\nQuestion: \" + flatten_and_concatenate(question) + \"\\n\\nAnswer: \" + flatten_and_concatenate(answer) + \"\\n\\nCandidate Answer: \" + flatten_and_concatenate(candidate) + \"\\n\\nScore: \"\n",
    "    chat = Chat.create(\"Score_Candidate_Answer_\" + str(count))\n",
    "\n",
    "    if (len(encoding.encode(prompt)) + 10) < 4097:\n",
    "        message = chat.ask(content=prompt)\n",
    "        print(message)\n",
    "    else:\n",
    "        print(\"Skipping datapoint\", count, \"because the prompt is too long for the model\")\n",
    "        skipped.append(count)\n",
    "        with open('skipped.json', 'w') as f:\n",
    "            json.dump(skipped, f)\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        score = int(message.content)\n",
    "    except (ValueError, TypeError):\n",
    "        print(\"Skipping datapoint\", count, \"because the scorer did not return a valid score\")\n",
    "        skipped.append(count)\n",
    "        with open('skipped.json', 'w') as f:\n",
    "            json.dump(skipped, f)\n",
    "        continue\n",
    "\n",
    "    if score != 0 and score != 1 and score != 2 and score != 3 and score != 4 and score != 5:\n",
    "        print(\"Skipping datapoint\", count, \"because the scorer did not return a valid score\")\n",
    "        skipped.append(count)\n",
    "        with open('skipped.json', 'w') as f:\n",
    "            json.dump(skipped, f)\n",
    "        continue\n",
    "\n",
    "    choices = None\n",
    "    if \"choices\" in datapoint and datapoint[\"choices\"] is not None:\n",
    "        choices = datapoint[\"choices\"]\n",
    "    explanation = None\n",
    "    if \"explanation\" in datapoint and datapoint[\"explanation\"] is not None:\n",
    "        explanation = datapoint[\"explanation\"]\n",
    "\n",
    "    datapoint_dict = {\n",
    "            \"question\": question,\n",
    "            \"answer\": answer,\n",
    "            \"BERTScore\": datapoint[\"BERTScore\"],\n",
    "            \"candidate_answer\": candidate,\n",
    "            \"confidence\": datapoint[\"confidence\"],\n",
    "            \"sol_id\": datapoint[\"sol_id\"],\n",
    "            \"interaction_id\": datapoint[\"interaction_id\"],\n",
    "            \"choices\": choices,\n",
    "            \"explanation\": explanation,\n",
    "            \"score\": score\n",
    "            }\n",
    "    \n",
    "    # Add the datapoint to the list\n",
    "    scored.append(datapoint_dict)\n",
    "\n",
    "    # Save the list of datapoints as a JSON file\n",
    "    with open('complete_complete_with_grades.json', 'w') as f:\n",
    "        json.dump(scored, f)\n",
    "\n",
    "    print(\"Tokens used: \", Chat.budget()['usage'] / Chat.budget()['limit'] * 100, \"%\")\n",
    "    print(\"Total tokens used: \", Chat.budget()['usage'])\n",
    "    available_tokens = Chat.budget()['limit'] - Chat.budget()['usage']\n",
    "    avg_tokens_per_datapoint = (Chat.budget()['usage'] - start_tokens_used) / count\n",
    "    print(\"Average remaining datapoints before running out of tokens: \", round(available_tokens / avg_tokens_per_datapoint))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "habitat",
   "language": "python",
   "name": "habitat"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
