{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook to obtain the third part of the context-enhanced datasets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we use ChatGPT (through the provided wrapper) to add context and relevant keywords to each datapoint of our datasets. These datasets are useful both to finetune the QA model, and the keyword retriever model. For scalability reason, the prompting has been divided in three notebooks, this is the third notebook of the three."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ./artifacts/gpt_wrapper-0.0.8-py3-none-any.whl\n",
      "Requirement already satisfied: requests in /Users/luca/miniconda3/lib/python3.9/site-packages (from gpt-wrapper==0.0.8) (2.28.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/luca/miniconda3/lib/python3.9/site-packages (from requests->gpt-wrapper==0.0.8) (1.26.14)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/luca/miniconda3/lib/python3.9/site-packages (from requests->gpt-wrapper==0.0.8) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/luca/miniconda3/lib/python3.9/site-packages (from requests->gpt-wrapper==0.0.8) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/luca/miniconda3/lib/python3.9/site-packages (from requests->gpt-wrapper==0.0.8) (2022.12.7)\n",
      "gpt-wrapper is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: tiktoken in /Users/luca/miniconda3/lib/python3.9/site-packages (0.4.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /Users/luca/miniconda3/lib/python3.9/site-packages (from tiktoken) (2022.4.24)\n",
      "Requirement already satisfied: requests>=2.26.0 in /Users/luca/miniconda3/lib/python3.9/site-packages (from tiktoken) (2.28.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/luca/miniconda3/lib/python3.9/site-packages (from requests>=2.26.0->tiktoken) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/luca/miniconda3/lib/python3.9/site-packages (from requests>=2.26.0->tiktoken) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/luca/miniconda3/lib/python3.9/site-packages (from requests>=2.26.0->tiktoken) (1.26.14)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/luca/miniconda3/lib/python3.9/site-packages (from requests>=2.26.0->tiktoken) (2.0.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: datasets in /Users/luca/miniconda3/lib/python3.9/site-packages (2.12.0)\n",
      "Requirement already satisfied: multiprocess in /Users/luca/miniconda3/lib/python3.9/site-packages (from datasets) (0.70.14)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /Users/luca/miniconda3/lib/python3.9/site-packages (from datasets) (0.3.6)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /Users/luca/miniconda3/lib/python3.9/site-packages (from datasets) (4.64.1)\n",
      "Requirement already satisfied: responses<0.19 in /Users/luca/miniconda3/lib/python3.9/site-packages (from datasets) (0.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in /Users/luca/miniconda3/lib/python3.9/site-packages (from datasets) (0.14.1)\n",
      "Requirement already satisfied: pandas in /Users/luca/miniconda3/lib/python3.9/site-packages (from datasets) (1.5.3)\n",
      "Requirement already satisfied: packaging in /Users/luca/miniconda3/lib/python3.9/site-packages (from datasets) (22.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/luca/miniconda3/lib/python3.9/site-packages (from datasets) (6.0)\n",
      "Requirement already satisfied: xxhash in /Users/luca/miniconda3/lib/python3.9/site-packages (from datasets) (3.2.0)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /Users/luca/miniconda3/lib/python3.9/site-packages (from datasets) (12.0.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /Users/luca/miniconda3/lib/python3.9/site-packages (from datasets) (2.28.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/luca/miniconda3/lib/python3.9/site-packages (from datasets) (1.23.5)\n",
      "Requirement already satisfied: aiohttp in /Users/luca/miniconda3/lib/python3.9/site-packages (from datasets) (3.8.4)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /Users/luca/miniconda3/lib/python3.9/site-packages (from datasets) (2023.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /Users/luca/miniconda3/lib/python3.9/site-packages (from aiohttp->datasets) (2.0.4)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/luca/miniconda3/lib/python3.9/site-packages (from aiohttp->datasets) (22.1.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/luca/miniconda3/lib/python3.9/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/luca/miniconda3/lib/python3.9/site-packages (from aiohttp->datasets) (1.9.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/luca/miniconda3/lib/python3.9/site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/luca/miniconda3/lib/python3.9/site-packages (from aiohttp->datasets) (1.3.3)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/luca/miniconda3/lib/python3.9/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/luca/miniconda3/lib/python3.9/site-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (4.4.0)\n",
      "Requirement already satisfied: filelock in /Users/luca/miniconda3/lib/python3.9/site-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (3.6.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/luca/miniconda3/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/luca/miniconda3/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (1.26.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/luca/miniconda3/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (3.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/luca/miniconda3/lib/python3.9/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/luca/miniconda3/lib/python3.9/site-packages (from pandas->datasets) (2022.7)\n",
      "Requirement already satisfied: six>=1.5 in /Users/luca/miniconda3/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: wikipedia-api in /Users/luca/miniconda3/lib/python3.9/site-packages (0.5.8)\n",
      "Requirement already satisfied: requests in /Users/luca/miniconda3/lib/python3.9/site-packages (from wikipedia-api) (2.28.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/luca/miniconda3/lib/python3.9/site-packages (from requests->wikipedia-api) (1.26.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/luca/miniconda3/lib/python3.9/site-packages (from requests->wikipedia-api) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/luca/miniconda3/lib/python3.9/site-packages (from requests->wikipedia-api) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/luca/miniconda3/lib/python3.9/site-packages (from requests->wikipedia-api) (2.0.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install artifacts/gpt_wrapper-0.0.8-py3-none-any.whl\n",
    "%pip install tiktoken\n",
    "%pip install datasets\n",
    "%pip install wikipedia-api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import gpt_wrapper\n",
    "from gpt_wrapper.chat import Chat\n",
    "import wikipediaapi\n",
    "import random\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_wrapper.api_key = \"API-KEY-REMOVED-FOR-PRIVACY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load JSON file\n",
    "def load_json(file):\n",
    "    with open(file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keywords_from_response(response):\n",
    "    # Define regular expressions to match the desired patterns\n",
    "    pattern_en = r'^EN\\.\\s*\\[?(.+)\\]?$'\n",
    "    pattern_fr = r'^FR\\.\\s*\\[?(.+)\\]?$'\n",
    "\n",
    "    # Check if the response matches the EN pattern\n",
    "    match_en = re.match(pattern_en, response)\n",
    "    if match_en:\n",
    "        return \"EN\", match_en.group(1)  # Return \"EN\" and the extracted keyword\n",
    "\n",
    "    # Check if the response matches the FR pattern\n",
    "    match_fr = re.match(pattern_fr, response)\n",
    "    if match_fr:\n",
    "        return \"FR\", match_fr.group(1)  # Return \"FR\" and the extracted keyword\n",
    "\n",
    "    # If the response doesn't match any pattern, return None\n",
    "    return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_and_concatenate(nested_list):\n",
    "    # If the input is a string, return it\n",
    "    if isinstance(nested_list, str):\n",
    "        return nested_list\n",
    "\n",
    "    # If the input is a list, apply the function to each element and concatenate the results\n",
    "    if isinstance(nested_list, list):\n",
    "        return ' '.join(flatten_and_concatenate(element) for element in nested_list)\n",
    "\n",
    "    # If the input is neither a string nor a list, return an empty string\n",
    "    return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########################################################\n",
      "Processed datapoint 31932 of 32110 ( 99.45 %)\n",
      "Success count:  25747\n",
      "########################################################\n",
      "ETA (minutes):  7.71\n",
      "ETA (hours):  0.13\n",
      "tokens remaining:  -697\n"
     ]
    },
    {
     "ename": "APIException",
     "evalue": "Team has reached its budget",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAPIException\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 54\u001b[0m\n\u001b[1;32m     52\u001b[0m chat \u001b[39m=\u001b[39m Chat\u001b[39m.\u001b[39mcreate(\u001b[39m\"\u001b[39m\u001b[39mKey_topics_\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(random\u001b[39m.\u001b[39mrandint(\u001b[39m0\u001b[39m, \u001b[39m1000000\u001b[39m)))\n\u001b[1;32m     53\u001b[0m query \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mGiven the following question, which concept or definition, if looked up on Wikipedia, would be most likely to help you answer it? Return a single concept, ideally corresponding to a Wikipedia page title. If the question is in English, answer in the following format: EN. [CONCEPT], if the question is in French, answer in the following format: FR. [CONCEPT]\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m question\n\u001b[0;32m---> 54\u001b[0m A \u001b[39m=\u001b[39m chat\u001b[39m.\u001b[39;49mask(content\u001b[39m=\u001b[39;49mquery)\n\u001b[1;32m     55\u001b[0m used_after \u001b[39m=\u001b[39m Chat\u001b[39m.\u001b[39mbudget()[\u001b[39m'\u001b[39m\u001b[39musage\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     56\u001b[0m language, keyword \u001b[39m=\u001b[39m extract_keywords_from_response(A\u001b[39m.\u001b[39mcontent)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/gpt_wrapper/chat.py:236\u001b[0m, in \u001b[0;36mChat.ask\u001b[0;34m(self, content, instruction, model_args)\u001b[0m\n\u001b[1;32m    230\u001b[0m response \u001b[39m=\u001b[39m api\u001b[39m.\u001b[39mpost(\n\u001b[1;32m    231\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m/api/chats/\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchat_id\u001b[39m}\u001b[39;00m\u001b[39m/messages\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    232\u001b[0m     json\u001b[39m=\u001b[39mdata,\n\u001b[1;32m    233\u001b[0m )\n\u001b[1;32m    235\u001b[0m \u001b[39mif\u001b[39;00m response\u001b[39m.\u001b[39mstatus_code \u001b[39m!=\u001b[39m \u001b[39m201\u001b[39m:\n\u001b[0;32m--> 236\u001b[0m     \u001b[39mraise\u001b[39;00m APIException(get_error_message(response))\n\u001b[1;32m    238\u001b[0m response_json \u001b[39m=\u001b[39m response\u001b[39m.\u001b[39mjson()\n\u001b[1;32m    240\u001b[0m \u001b[39mif\u001b[39;00m response_json[\u001b[39m\"\u001b[39m\u001b[39mover_soft_limit\u001b[39m\u001b[39m\"\u001b[39m]:\n",
      "\u001b[0;31mAPIException\u001b[0m: Team has reached its budget"
     ]
    }
   ],
   "source": [
    "# Create an empty list to store the datapoints\n",
    "QandA = []\n",
    "start_index = 0\n",
    "success_count = 0\n",
    "\n",
    "#solutions = load_json('DATASETS/gen_dataset_modern_nlm_ai.json')\n",
    "#solutions = solutions + load_json('DATASETS/gen_dataset_modern_nlm_class_dataset.json')\n",
    "#solutions = solutions + load_json('DATASETS/gen_dataset_modern_nlm_eli5_questions_answers.json')\n",
    "#solutions = solutions + load_json('DATASETS/gen_dataset_modern_nlm_hh_questions_answers.json')\n",
    "#solutions = solutions + load_json('DATASETS/gen_dataset_modern_nlm_sciq.json')\n",
    "#solutions = solutions + load_json('DATASETS/gen_dataset_modern_nlm_stack_exchange.json')\n",
    "solutions = load_json('DATASETS/gen_dataset_modern_nlm_synthetic_questions_answers.json')\n",
    "solutions = solutions + load_json('DATASETS/gen_dataset_modern_nlm_texas_SFT.json')\n",
    "solutions = solutions + load_json('DATASETS/gen_modern_nlm_stack_exchange.json')\n",
    "\n",
    "try:\n",
    "    # Load the previous list of datapoints (useful to resume the process if it was interrupted)\n",
    "    with open('Definitions_3.json', 'r') as f:\n",
    "        QandA = json.load(f)\n",
    "        success_count = len(QandA)\n",
    "\n",
    "    # Find the highest ID among the existing datapoints\n",
    "    start_index = max([int(item['ID']) for item in QandA]) + 1 if QandA else 0\n",
    "\n",
    "    if start_index == 0:\n",
    "        print(\"Error: No datapoints found in the existing datasets.\")\n",
    "    else:\n",
    "        print(\"Starting index:\", start_index)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Failed to load previous data: {e}\")\n",
    "    \n",
    "data_len = len(solutions)\n",
    "start_time = time.time()\n",
    "elapsed_times = []\n",
    "token_costs = []\n",
    "\n",
    "for index_q, datapoint in enumerate(solutions[start_index:], start=start_index):\n",
    "\n",
    "    iteration_start_time = time.time()\n",
    "    finished = False\n",
    "    remaining_trials = 4\n",
    "\n",
    "    if 'question' in datapoint:\n",
    "        question = datapoint['question']\n",
    "        if 'choices' in datapoint and datapoint['choices'] is not None:\n",
    "            question += flatten_and_concatenate(datapoint['choices'])\n",
    "\n",
    "        if question is not None:\n",
    "            while remaining_trials > 0 and finished is False:\n",
    "                remaining_trials -= 1\n",
    "                chat = Chat.create(\"Key_topics_\" + str(random.randint(0, 1000000)))\n",
    "                query = \"Given the following question, which concept or definition, if looked up on Wikipedia, would be most likely to help you answer it? Return a single concept, ideally corresponding to a Wikipedia page title. If the question is in English, answer in the following format: EN. [CONCEPT], if the question is in French, answer in the following format: FR. [CONCEPT]\\n\\n\" + question\n",
    "                A = chat.ask(content=query)\n",
    "                used_after = Chat.budget()['usage']\n",
    "                language, keyword = extract_keywords_from_response(A.content)\n",
    "                if keyword is not None:\n",
    "                    if debug:\n",
    "                        print(\"Keyword: \", keyword)\n",
    "                else:\n",
    "                    if debug:\n",
    "                        print(f\"Response was not in the expected format\")\n",
    "\n",
    "                if keyword is not None:\n",
    "                    # Create a Wikipedia API client\n",
    "                    wiki_wiki = wikipediaapi.Wikipedia(language)\n",
    "\n",
    "                    # Retrieve and print the main definition for each keyword\n",
    "                    page = wiki_wiki.page(keyword)\n",
    "                    if page.exists():\n",
    "                        if \"may refer to\" in page.text or \"plusieurs concepts\" in page.text or \"dans les articles suivants\" in page.text or \"Suivant le contexte, le terme\" in page.text:\n",
    "                            if debug:\n",
    "                                print(f\"Skipping disambiguation page for '{keyword}'\")\n",
    "                        else:\n",
    "                            if debug:\n",
    "                                print(f\"Main definition for '{keyword}':\")\n",
    "                                print(page.summary)\n",
    "\n",
    "                            datapoint = {\n",
    "                                \"Language\": language,\n",
    "                                \"Question\": question,\n",
    "                                \"Keyword\": keyword,\n",
    "                                \"Wiki_Summary\": page.summary,\n",
    "                                \"ID\": index_q\n",
    "                            }\n",
    "                            if debug:\n",
    "                                print(datapoint)\n",
    "\n",
    "                            QandA.append(datapoint)\n",
    "                            finished = True\n",
    "                            success_count += 1\n",
    "                    else:\n",
    "                        if debug:\n",
    "                            print(f\"No webpage found for '{keyword}'\")\n",
    "\n",
    "    # Save the list of datapoints as a JSON file\n",
    "    with open('Definitions_3.json', 'w') as f:\n",
    "        json.dump(QandA, f)\n",
    "\n",
    "    iteration_end_time = time.time()\n",
    "\n",
    "    elapsed_time = iteration_end_time - iteration_start_time\n",
    "    elapsed_times.append(elapsed_time)\n",
    "\n",
    "    average_time_per_datapoint = sum(elapsed_times) / len(elapsed_times)\n",
    "    remaining_datapoints = data_len - index_q\n",
    "    estimated_time_remaining = remaining_datapoints * average_time_per_datapoint\n",
    "\n",
    "    clear_output()\n",
    "    print(\"########################################################\")\n",
    "    print(\"Processed datapoint\", index_q, \"of\", (data_len - 1), \"(\", round(index_q/(data_len-1)*100, 2), \"%)\")\n",
    "    print(\"Success count: \", success_count)\n",
    "    print(\"########################################################\")\n",
    "\n",
    "    print(\"ETA (minutes): \", round(estimated_time_remaining/60, 2))\n",
    "    print(\"ETA (hours): \", round(estimated_time_remaining/3600, 2))\n",
    "    tokens_remaining = Chat.budget()['limit'] - used_after\n",
    "    print(\"tokens remaining: \", tokens_remaining)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
