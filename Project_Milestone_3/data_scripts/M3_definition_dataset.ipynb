{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ./artifacts/gpt_wrapper-0.0.8-py3-none-any.whl\n",
      "Requirement already satisfied: requests in /Users/luca/miniconda3/lib/python3.9/site-packages (from gpt-wrapper==0.0.8) (2.28.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/luca/miniconda3/lib/python3.9/site-packages (from requests->gpt-wrapper==0.0.8) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/luca/miniconda3/lib/python3.9/site-packages (from requests->gpt-wrapper==0.0.8) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/luca/miniconda3/lib/python3.9/site-packages (from requests->gpt-wrapper==0.0.8) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/luca/miniconda3/lib/python3.9/site-packages (from requests->gpt-wrapper==0.0.8) (1.26.14)\n",
      "gpt-wrapper is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: tiktoken in /Users/luca/miniconda3/lib/python3.9/site-packages (0.4.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /Users/luca/miniconda3/lib/python3.9/site-packages (from tiktoken) (2022.4.24)\n",
      "Requirement already satisfied: requests>=2.26.0 in /Users/luca/miniconda3/lib/python3.9/site-packages (from tiktoken) (2.28.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/luca/miniconda3/lib/python3.9/site-packages (from requests>=2.26.0->tiktoken) (1.26.14)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/luca/miniconda3/lib/python3.9/site-packages (from requests>=2.26.0->tiktoken) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/luca/miniconda3/lib/python3.9/site-packages (from requests>=2.26.0->tiktoken) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/luca/miniconda3/lib/python3.9/site-packages (from requests>=2.26.0->tiktoken) (3.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: datasets in /Users/luca/miniconda3/lib/python3.9/site-packages (2.12.0)\n",
      "Requirement already satisfied: multiprocess in /Users/luca/miniconda3/lib/python3.9/site-packages (from datasets) (0.70.14)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /Users/luca/miniconda3/lib/python3.9/site-packages (from datasets) (4.64.1)\n",
      "Requirement already satisfied: pandas in /Users/luca/miniconda3/lib/python3.9/site-packages (from datasets) (1.5.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/luca/miniconda3/lib/python3.9/site-packages (from datasets) (6.0)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /Users/luca/miniconda3/lib/python3.9/site-packages (from datasets) (0.3.6)\n",
      "Requirement already satisfied: requests>=2.19.0 in /Users/luca/miniconda3/lib/python3.9/site-packages (from datasets) (2.28.1)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /Users/luca/miniconda3/lib/python3.9/site-packages (from datasets) (2023.5.0)\n",
      "Requirement already satisfied: aiohttp in /Users/luca/miniconda3/lib/python3.9/site-packages (from datasets) (3.8.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in /Users/luca/miniconda3/lib/python3.9/site-packages (from datasets) (0.14.1)\n",
      "Requirement already satisfied: responses<0.19 in /Users/luca/miniconda3/lib/python3.9/site-packages (from datasets) (0.18.0)\n",
      "Requirement already satisfied: packaging in /Users/luca/miniconda3/lib/python3.9/site-packages (from datasets) (22.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/luca/miniconda3/lib/python3.9/site-packages (from datasets) (1.23.5)\n",
      "Requirement already satisfied: xxhash in /Users/luca/miniconda3/lib/python3.9/site-packages (from datasets) (3.2.0)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /Users/luca/miniconda3/lib/python3.9/site-packages (from datasets) (12.0.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/luca/miniconda3/lib/python3.9/site-packages (from aiohttp->datasets) (1.9.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/luca/miniconda3/lib/python3.9/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/luca/miniconda3/lib/python3.9/site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/luca/miniconda3/lib/python3.9/site-packages (from aiohttp->datasets) (1.3.3)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /Users/luca/miniconda3/lib/python3.9/site-packages (from aiohttp->datasets) (2.0.4)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/luca/miniconda3/lib/python3.9/site-packages (from aiohttp->datasets) (22.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/luca/miniconda3/lib/python3.9/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/luca/miniconda3/lib/python3.9/site-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (4.4.0)\n",
      "Requirement already satisfied: filelock in /Users/luca/miniconda3/lib/python3.9/site-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (3.6.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/luca/miniconda3/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/luca/miniconda3/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/luca/miniconda3/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (3.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/luca/miniconda3/lib/python3.9/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/luca/miniconda3/lib/python3.9/site-packages (from pandas->datasets) (2022.7)\n",
      "Requirement already satisfied: six>=1.5 in /Users/luca/miniconda3/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: wikipedia-api in /Users/luca/miniconda3/lib/python3.9/site-packages (0.5.8)\n",
      "Requirement already satisfied: requests in /Users/luca/miniconda3/lib/python3.9/site-packages (from wikipedia-api) (2.28.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/luca/miniconda3/lib/python3.9/site-packages (from requests->wikipedia-api) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/luca/miniconda3/lib/python3.9/site-packages (from requests->wikipedia-api) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/luca/miniconda3/lib/python3.9/site-packages (from requests->wikipedia-api) (1.26.14)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/luca/miniconda3/lib/python3.9/site-packages (from requests->wikipedia-api) (2.0.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install artifacts/gpt_wrapper-0.0.8-py3-none-any.whl\n",
    "%pip install tiktoken\n",
    "%pip install datasets\n",
    "%pip install wikipedia-api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import gpt_wrapper\n",
    "from gpt_wrapper.chat import Chat\n",
    "import wikipediaapi\n",
    "import random\n",
    "import time\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_wrapper.api_key = \"KEY_REMOVED_FOR_PRIVACY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load JSON file\n",
    "def load_json(file):\n",
    "    with open(file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "solutions = load_json('solutions_v1.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keywords_from_response(response):\n",
    "    keywords = []\n",
    "    for line in response.split('\\n'):\n",
    "        if line.strip() != \"\":\n",
    "            try:\n",
    "                number, keyword = line.split(\". \", 1)\n",
    "                # verify that the number is an integer\n",
    "                int(number)\n",
    "                keywords.append(keyword.strip())\n",
    "            except ValueError:\n",
    "                # the line was not in the expected format\n",
    "                return None\n",
    "    return keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts_FR = [\n",
    "    \"Veuillez définir le concept de\",\n",
    "    \"Pouvez-vous donner une définition de\",\n",
    "    \"Quelle est la signification de\",\n",
    "    \"J'aimerais connaître la définition de\",\n",
    "    \"Pouvez-vous expliquer ce que l'on entend par\",\n",
    "    \"Définissez le terme\",\n",
    "    \"Que signifie le terme\",\n",
    "    \"Pouvez-vous me donner une brève explication de\",\n",
    "    \"Veuillez décrire le concept de\",\n",
    "    \"Quelle est la définition de\"\n",
    "]\n",
    "\n",
    "prompts_EN = [\n",
    "    \"Please define the concept of\",\n",
    "    \"Can you provide a definition for\",\n",
    "    \"What is the meaning of\",\n",
    "    \"I would like to know the definition of\",\n",
    "    \"Could you explain what is meant by\",\n",
    "    \"Define the term\",\n",
    "    \"What does the term\",\n",
    "    \"Can you give me a brief explanation of\",\n",
    "    \"Please describe the concept of\",\n",
    "    \"What is the definition of\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Question': 'Pouvez-vous me donner une brève explication de Rayonnement électromagnétique', 'Answer': \"Le rayonnement électromagnétique est  une forme de transfert d'énergie linéaire. La lumière visible est un rayonnement électromagnétique, mais ne constitue qu'une petite tranche du large spectre électromagnétique. La propagation de ce rayonnement, d'une ou plusieurs particules, donne lieu à de nombreux phénomènes comme l'atténuation, l'absorption, la diffraction et la réfraction, le décalage vers le rouge, les interférences, les échos, les parasites électromagnétiques et les effets biologiques.\\nLe rayonnement électromagnétique peut être décrit de manière corpusculaire comme la propagation de photons (boson vecteur de l'interaction électromagnétique), ou de manière ondulatoire comme une onde électromagnétique. Il se manifeste sous la forme d'un champ électrique couplé à un champ magnétique.\", 'ID': 2}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "datapoint = {\n",
    "    \"Question\": \"Pouvez-vous me donner une br\\u00e8ve explication de Rayonnement \\u00e9lectromagn\\u00e9tique\",\n",
    "    \"Answer\": \"Le rayonnement \\u00e9lectromagn\\u00e9tique est  une forme de transfert d'\\u00e9nergie lin\\u00e9aire. La lumi\\u00e8re visible est un rayonnement \\u00e9lectromagn\\u00e9tique, mais ne constitue qu'une petite tranche du large spectre \\u00e9lectromagn\\u00e9tique. La propagation de ce rayonnement, d'une ou plusieurs particules, donne lieu \\u00e0 de nombreux ph\\u00e9nom\\u00e8nes comme l'att\\u00e9nuation, l'absorption, la diffraction et la r\\u00e9fraction, le d\\u00e9calage vers le rouge, les interf\\u00e9rences, les \\u00e9chos, les parasites \\u00e9lectromagn\\u00e9tiques et les effets biologiques.\\nLe rayonnement \\u00e9lectromagn\\u00e9tique peut \\u00eatre d\\u00e9crit de mani\\u00e8re corpusculaire comme la propagation de photons (boson vecteur de l'interaction \\u00e9lectromagn\\u00e9tique), ou de mani\\u00e8re ondulatoire comme une onde \\u00e9lectromagn\\u00e9tique. Il se manifeste sous la forme d'un champ \\u00e9lectrique coupl\\u00e9 \\u00e0 un champ magn\\u00e9tique.\",\n",
    "    \"ID\": 2\n",
    "}\n",
    "\n",
    "json_string = json.dumps(datapoint, ensure_ascii=False)\n",
    "decoded_datapoint = json.loads(json_string)\n",
    "print(decoded_datapoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########################################################\n",
      "Processed datapoint 4449 of 4449 ( 100.0 %)\n",
      "########################################################\n",
      "EN datapoints:  4  out of  5  keywords | FR datapoints:  4  out of  5  keywords\n",
      "ETA (minutes):  0.13 | Average tokens used per iteration:  212.2934451219512\n",
      "Estimated iterations remaining:  1037.6439078157032\n"
     ]
    }
   ],
   "source": [
    "# Create an empty list to store the datapoints\n",
    "QandA_EN = []\n",
    "QandA_FR = []\n",
    "start_index = 0\n",
    "\n",
    "try:\n",
    "    # Load the previous list of datapoints (useful to resume the process if it was interrupted)\n",
    "    with open('QandA_definitions_EN.json', 'r') as f:\n",
    "        QandA_EN = json.load(f)\n",
    "    with open('QandA_definitions_FR.json', 'r') as f:\n",
    "        QandA_FR = json.load(f)\n",
    "\n",
    "    # Find the highest ID among the existing datapoints\n",
    "    start_index_EN = max([int(item['ID']) for item in QandA_EN]) + 1 if QandA_EN else -1\n",
    "    start_index_FR = max([int(item['ID']) for item in QandA_FR]) + 1 if QandA_FR else -1\n",
    "\n",
    "    # Check if the starting indices match\n",
    "    if start_index_EN != start_index_FR:\n",
    "        print(\"Starting indices don't match between English and French datasets.\")\n",
    "        if start_index_EN > start_index_FR:\n",
    "            start_index = start_index_EN\n",
    "        else:\n",
    "            start_index = start_index_FR\n",
    "        print(\"Starting index:\", start_index)\n",
    "    elif start_index_EN == -1:\n",
    "        print(\"Error: No datapoints found in the existing datasets.\")\n",
    "    else:\n",
    "        start_index = start_index_EN\n",
    "        print(\"Starting index:\", start_index)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Failed to load previous data: {e}\")\n",
    "    \n",
    "data_len = len(solutions)\n",
    "start_time = time.time()\n",
    "elapsed_times = []\n",
    "token_costs = []\n",
    "\n",
    "for i, datapoint in enumerate(solutions[start_index:], start_index):\n",
    "\n",
    "    iteration_start_time = time.time()\n",
    "    cost = 0\n",
    "\n",
    "    question = datapoint['question']\n",
    "    if question is not None:\n",
    "        max_retries = 3\n",
    "        for j in range(max_retries):\n",
    "            chat = Chat.create(\"EN_Key_topics_\" + str(i) + \"_\" + str(j))\n",
    "            query = \"Please list the key topics related to the following question, each one numbered and on a new line. Each topic should be SHORT (a couple of words per topic MAXIMUM) and ideally correspond to a Wikipedia page title (DO NOT INCLUDE LINKS, DO NOT INCLUDE ABBREVIATIONS). Regardless of the language of the question, return keywords in ENGLISH:\\n\\n\" + question\n",
    "            used_before = Chat.budget()['usage']\n",
    "            A = chat.ask(content=query)\n",
    "            used_after = Chat.budget()['usage']\n",
    "            cost = used_after - used_before\n",
    "            \n",
    "            keywords = extract_keywords_from_response(A.content)\n",
    "            if keywords is not None:\n",
    "                count_keywords_EN = len(keywords)\n",
    "                if debug:\n",
    "                    print(\"Keywords:\", keywords)\n",
    "                    print(\"Number of keywords obtained: \", len(keywords))\n",
    "                break\n",
    "            else:\n",
    "                if debug:\n",
    "                    print(f\"Response was not in the expected format, retrying ({i+1}/{max_retries})\")\n",
    "\n",
    "        # Create a Wikipedia API client\n",
    "        wiki_wiki = wikipediaapi.Wikipedia('en')\n",
    "\n",
    "        # Retrieve and print the main definition for each keyword\n",
    "        datapoint_count_EN = 0\n",
    "        if keywords is not None:\n",
    "            for keyword in keywords:\n",
    "                page = wiki_wiki.page(keyword)\n",
    "                if page.exists():\n",
    "                    if \"may refer to\" in page.text:\n",
    "                        if debug:\n",
    "                            print(f\"Skipping disambiguation page for '{keyword}'\")\n",
    "                    else:\n",
    "                        if debug:\n",
    "                            print(f\"Main definition for '{keyword}':\")\n",
    "                            print(page.summary)\n",
    "\n",
    "                        prompt = random.choice(prompts_EN)\n",
    "                        datapoint_EN = {\n",
    "                            \"Question\": f\"{prompt} {keyword}\",\n",
    "                            \"Answer\": page.summary,\n",
    "                            \"ID\": i,\n",
    "                            \"Keyword\": keyword,\n",
    "                            \"Language\": \"EN\"\n",
    "                        }\n",
    "                        if debug:\n",
    "                            print(datapoint_EN)\n",
    "                        datapoint_count_EN += 1\n",
    "\n",
    "                        QandA_EN.append(datapoint_EN)\n",
    "                else:\n",
    "                    if debug:\n",
    "                        print(f\"No webpage found for '{keyword}'\")\n",
    "\n",
    "        #############################################################################\n",
    "        \n",
    "        max_retries = 3\n",
    "        for j in range(max_retries):\n",
    "            chat = Chat.create(\"FR_Key_topics_\" + str(i) + \"_\" + str(j))\n",
    "            query = \"Please list the key topics related to the following question, each one numbered and on a new line. Each topic should be SHORT (a couple of words per topic MAXIMUM) and ideally correspond to a Wikipedia page title (DO NOT INCLUDE LINKS, DO NOT INCLUDE ABBREVIATIONS). Regardless of the language of the question, return keywords in FRENCH:\\n\\n\" + question\n",
    "            used_before = Chat.budget()['usage']\n",
    "            A = chat.ask(content=query)\n",
    "            used_after = Chat.budget()['usage']\n",
    "            cost = cost + (used_after - used_before)\n",
    "            \n",
    "            keywords = extract_keywords_from_response(A.content)\n",
    "            if keywords is not None:\n",
    "                count_keywords_FR = len(keywords)\n",
    "                if debug:\n",
    "                    print(\"Keywords:\", keywords)\n",
    "                    print(\"Number of keywords obtained: \", len(keywords))\n",
    "                break\n",
    "            else:\n",
    "                if debug:\n",
    "                    print(f\"Response was not in the expected format, retrying ({i+1}/{max_retries})\")\n",
    "\n",
    "        # Create a Wikipedia API client\n",
    "        wiki_wiki = wikipediaapi.Wikipedia('fr')\n",
    "\n",
    "        # Retrieve and print the main definition for each keyword\n",
    "        datapoint_count_FR = 0\n",
    "        if keywords is not None:\n",
    "            for keyword in keywords:\n",
    "                page = wiki_wiki.page(keyword)\n",
    "                if page.exists():\n",
    "                    if \"plusieurs concepts\" in page.text or \"dans les articles suivants\" in page.text or \"Suivant le contexte, le terme\" in page.text:\n",
    "                        if debug:\n",
    "                            print(f\"Skipping disambiguation page for '{keyword}'\")\n",
    "                    else:\n",
    "                        if debug:\n",
    "                            print(f\"Main definition for '{keyword}':\")\n",
    "                            print(page.summary)\n",
    "\n",
    "                        prompt = random.choice(prompts_FR)\n",
    "                        datapoint_FR = {\n",
    "                            \"Question\": f\"{prompt} {keyword}\",\n",
    "                            \"Answer\": page.summary,\n",
    "                            \"ID\": i,\n",
    "                            \"Keyword\": keyword,\n",
    "                            \"Language\": \"FR\"\n",
    "                        }\n",
    "                        if debug:\n",
    "                            print(datapoint_FR)\n",
    "                        datapoint_count_FR += 1\n",
    "\n",
    "                        QandA_FR.append(datapoint_FR)\n",
    "                else:\n",
    "                    if debug:\n",
    "                        print(f\"No webpage found for '{keyword}'\")\n",
    "\n",
    "        # Save the list of datapoints as a JSON file\n",
    "        with open('QandA_definitions_EN.json', 'w') as f:\n",
    "            json.dump(QandA_EN, f)\n",
    "        with open('QandA_definitions_FR.json', 'w') as f:\n",
    "            json.dump(QandA_FR, f)\n",
    "\n",
    "        iteration_end_time = time.time()\n",
    "\n",
    "        elapsed_time = iteration_end_time - iteration_start_time\n",
    "        elapsed_times.append(elapsed_time)\n",
    "\n",
    "        token_cost = used_after - used_before\n",
    "        token_costs.append(token_cost)\n",
    "\n",
    "        average_time_per_datapoint = sum(elapsed_times) / len(elapsed_times)\n",
    "        remaining_datapoints = data_len - i\n",
    "        estimated_time_remaining = remaining_datapoints * average_time_per_datapoint\n",
    "\n",
    "        clear_output()\n",
    "        print(\"########################################################\")\n",
    "        print(\"Processed datapoint\", i, \"of\", (data_len - 1), \"(\", round(i/(data_len-1)*100, 2), \"%)\")\n",
    "        print(\"########################################################\")\n",
    "\n",
    "        print(\"EN datapoints: \", datapoint_count_EN, \" out of \", count_keywords_EN, \" keywords | FR datapoints: \", datapoint_count_FR, \" out of \", count_keywords_FR, \" keywords\")\n",
    "        print(\"ETA (minutes): \", round(estimated_time_remaining/60, 2), \"| Average tokens used per iteration: \", sum(token_costs) / len(token_costs))\n",
    "        tokens_remaining = Chat.budget()['limit'] - used_after\n",
    "        estimated_iterations_remaining = tokens_remaining / ((sum(token_costs) / len(token_costs)))\n",
    "        print(\"Estimated iterations remaining: \", estimated_iterations_remaining)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
